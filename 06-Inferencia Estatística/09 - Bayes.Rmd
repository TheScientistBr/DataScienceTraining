---
title       : Inferência Estatística
subtitle    : Bayes
author      : Delermando Branquinho Filho
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---



## Análise bayesiana
- Estatísticas bayesianas postulam a *prioi* sobre o parâmetro de interesse
- Todas as inferências são então realizadas na distribuição de o parâmetro dado os dados, chamado de a *posteriori*
- Em geral,

  $$
  \mbox{A Posteriori} \propto \mbox{Probabilidade} \times \mbox{A Priori}
  $$

- Portanto, como vimos nos testes diagnósticos, o fator pelo qual nossas crenças prévias são atualizadas para produzir conclusões à luz dos dados recolhidos

---

## Especificação prévia
- A distribuição beta é a probabilidade a priori para parâmetros entre $0$ e $1$.
- A densidade beta depende de dois parâmetros $\alpha$ e $\beta$

$$
\frac{\Gamma(\alpha +  \beta)}{\Gamma(\alpha)\Gamma(\beta)}
 p ^ {\alpha - 1} (1 - p) ^ {\beta - 1} ~~~~\mbox{para} ~~ 0 \leq p \leq 1
$$
- A média da densidade beta é $\alpha / (\alpha + \beta)$  
- A variação da densidade beta é
$$\frac {\alpha \beta} {(\alpha + \beta) ^ 2 (\alpha + \beta + 1)}$$
- A densidade uniforme é o caso especial onde $\alpha = \beta = 1$

---

```r
## Explorando a densidade beta
library(manipulate)
pvals <- seq(0.01, 0.99, length = 1000)
manipulate(
    plot(pvals, dbeta(pvals, alpha, beta), type = "l", lwd = 3, frame = FALSE),
    alpha = slider(0.01, 10, initial = 1, step = .5),
    beta = slider(0.01, 10, initial = 1, step = .5)
    )
```


```{r echo=FALSE}
x=seq(0,1.5,length=100)
y=dbeta(x,7,2)
plot(x,y, type="l", col="blue")
```

---

## Posteriori

- Suponha que escolhemos valores de $\alpha$ e $\beta$ para que o beta anterior é indicativo do nosso grau de crença em relação a $p$ na ausência de dados
- Então usando a regra que
  $$
  \mbox{Posterior} \propto \mbox{Likelihood} \times \mbox{Prior}
  $$
  E jogando fora qualquer coisa que não depende de $p$, temos que

$$
\begin{align}
\mbox{Posterior} &\propto  p^x(1 - p)^{n-x} \times p^{\alpha -1} (1 - p)^{\beta - 1} \\
                 &  =      p^{x + \alpha - 1} (1 - p)^{n - x + \beta - 1}
\end{align}
$$
- Esta densidade é apenas outra densidade beta com parâmetros
  $\tilde \alpha = x + \alpha$ and $\tilde \beta = n - x + \beta$


---
## Média posteriori

$$
\begin{align}
E[p ~|~ X] & =   \frac{\tilde \alpha}{\tilde \alpha + \tilde \beta}\\ \\
& =  \frac{x + \alpha}{x + \alpha + n - x + \beta}\\ \\
& =  \frac{x + \alpha}{n + \alpha + \beta} \\ \\
& =  \frac{x}{n} \times \frac{n}{n + \alpha + \beta} + \frac{\alpha}{\alpha + \beta} \times \frac{\alpha + \beta}{n + \alpha + \beta} \\ \\
& =  \mbox{MLE} \times \pi + \mbox{media a priori} \times (1 - \pi)
\end{align}
$$

MLE -> Maximum likelihood estimation

---

## Exemplo

- Suponha que em uma amostra aleatória de uma população em risco
$13$ de $20$ indivíduos tinham hipertensão. Estimar a prevalência
De hipertensão nesta população.
- $x = 13$ e $n = 20$
- Considere uma prioridade uniforme, $\alpha = \beta = 1$
- O posterior é proporcional a (ver fórmula acima)

$$
p^{x + \alpha - 1} (1 - p)^{n - x + \beta - 1} = p^x (1 - p)^{n-x}
$$
Isto é, para o uniforme prévio, o posterior é a probabilidade
- Considere a instância onde $\alpha = \beta = 2$ (lembre-se disso antes é a curva em torno do ponto $.5$) a posteriori é
$$
p^{x + \alpha - 1} (1 - p)^{n - x + \beta - 1} = p^{x + 1} (1 - p)^{n-x + 1}
$$
- A "priori", que tem alguns benefícios teóricos, coloca $\alpha = \beta = .5$

---

```r
pvals <- seq(0.01, 0.99, length = 1000)
x <- 13; n <- 20
myPlot <- function(alpha, beta){
    plot(0 : 1, 0 : 1, type = "n", xlab = "p", ylab = "", frame = FALSE)
    lines(pvals, dbeta(pvals, alpha, beta) / max(dbeta(pvals, alpha, beta)), 
            lwd = 3, col = "darkred")
    lines(pvals, dbinom(x,n,pvals) / dbinom(x,n,x/n), lwd = 3, col = "darkblue")
    lines(pvals, dbeta(pvals, alpha+x, beta+(n-x)) / max(dbeta(pvals, alpha+x, beta+(n-x))),
        lwd = 3, col = "darkgreen")
    title("red=prior,green=posterior,blue=likelihood")
}
manipulate(
    myPlot(alpha, beta),
    alpha = slider(0.01, 100, initial = 1, step = .5),
    beta = slider(0.01, 100, initial = 1, step = .5)
    )
```

---
## Intervalos de confiança
- Um intervalo bayesiano credível é o análogo bayesiano de um intervalo de confiança 
- A $95\%$ intervalo crível, $[a, b]$ iria satisfazer
  $$
  P(p \in [a, b] ~|~ x) = .95
  $$
- Os melhores intervalos críveis cortar o posterior com uma horizontal linha da mesma forma que fizemos para verossimilhança
- Estes são chamados de maior intervalo de densidade a posteriori   (HPD - highest posterior density)

---

## Obtendo HPD intervalos para este exemplo
- Instale o pacote \texttt{binom}, com o comando `install.package(binom)`

Dá o intervalo HPD.
- O nível credível padrão é $95\%$.


**HPD** -> Highest Posterior Density 

**Exemplo**

```{r}
library(binom)
binom.bayes(13, 20, type = "highest")
```


---

```r
pvals <- seq(0.01, 0.99, length = 1000)
x <- 13; n <- 20
myPlot2 <- function(alpha, beta, cl){
    plot(pvals, dbeta(pvals, alpha+x, beta+(n-x)), type = "l", lwd = 3,
    xlab = "p", ylab = "", frame = FALSE)
    out <- binom.bayes(x, n, type = "highest", 
        prior.shape1 = alpha, 
        prior.shape2 = beta, 
        conf.level = cl)
    p1 <- out$lower; p2 <- out$upper
    lines(c(p1, p1, p2, p2), c(0, dbeta(c(p1, p2), alpha+x, beta+(n-x)), 0), 
        type = "l", lwd = 3, col = "darkred")
}
manipulate(
    myPlot2(alpha, beta, cl),
    alpha = slider(0.01, 10, initial = 1, step = .5),
    beta = slider(0.01, 10, initial = 1, step = .5),
    cl = slider(0.01, 0.99, initial = 0.95, step = .01)
    )
```

