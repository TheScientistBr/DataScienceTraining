---
title       : Modelos de Regressão
subtitle    : Variação Residual
author      : Delermando Branquinho Filho
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---

## Resíduos
- Modelo $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ onde $\epsilon_i \sim N (0, \sigma ^ 2)$.
- Resultado observado $i$ é $Y_i$ no valor preditor $X_i$
- Resultado previsto $i$ is $\hat Y_i$ no predictor valuve $X_i$ é

$$
\hat Y_i = \hat \beta_0 + \hat \beta_1 X_i
$$

- Residual, o entre o resultado observado e previsto

$$
E_i = Y_i - \hat Y_i
$$

- A distância vertical entre o ponto de dados observado ea linha de regressão
- Mínimos quadrados minimiza $\sum_ {i = 1} ^ n e_i ^ 2$
- O $e_i$ pode ser pensado como estimativas do $\epsilon_i$.

---

## Propriedades dos resíduos
- $E [e_i] = 0$.
- Se uma interceptação estiver incluída, $\sum {i = 1} ^ n e_i = 0$
- Se uma variável regressora, $X_i$, estiver incluída no modelo $\sum_ {i = 1} ^ n e_i X_i = 0$.
- Residuais são úteis para investigar o ajuste pobre do modelo.
- Os resíduos positivos estão acima da linha, os resíduos negativos estão abaixo.
- Resíduos podem ser pensados como os resultados ($Y$) com o preditor linear do ($X$) removido.
- Um diferencia a variação residual (variação após a remoção o preditor) da variação sistemática (variação explicada pelo modelo de regressão).
- As parcelas residuais destacam o ajuste inadequado do modelo.

---

## Codigo

```{r message=FALSE}
library(UsingR,quietly = TRUE); 
data(diamond)
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
e <- resid(fit)
yhat <- predict(fit)
max(abs(e -(y - yhat)))
max(abs(e - (y - coef(fit)[1] - coef(fit)[2] * x)))
```

---

## Residuais são os comprimentos assinalados com linhas vermelhas

```{r, echo = FALSE, fig.height=5, fig.width=5}
plot(diamond$carat, diamond$price,  
     xlab = "Mass (carats)", 
     ylab = "Price (SIN $)", 
     bg = "lightblue", 
     col = "black", cex = 1.1, pch = 21,frame = FALSE)
abline(fit, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(y[i], yhat[i]), col = "red" , lwd = 2)
```

---

## Residuos versus X

```{r, echo = FALSE, fig.height=5, fig.width=5}
plot(diamond$carat, e,  
     xlab = "Mass (carats)", 
     ylab = "Residuals (SIN $)", 
     bg = "lightblue", 
     col = "black", cex = 1.1, pch = 21,frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 2)
```

---

## Non-linear data

```{r, echo = TRUE, fig.height=5, fig.width=5}
x <- runif(100, -3, 3); y <- x + sin(x) + rnorm(100, sd = .2); 
plot(x, y); abline(lm(y ~ x))
```

---

```{r, echo = TRUE, fig.height=5, fig.width=5}
plot(x, resid(lm(y ~ x))); 
abline(h = 0)
```

---
## Heteroscedasticidade

Heteroscedasticidade ou Heterocedasticidade é o fenômeno estatístico que ocorre quando o modelo de hipótese matemático apresenta variâncias para $Y$ e $X(X_1, X_2, X_3, \ldots , X_n)$ não iguais para todas as observações.

Esta hipótese do Modelo Clássico de Regressão Linear, pressupõe que a variância de cada termo de perturbação $u_i$, condicional aos valores escolhidos das variáveis explicativas, é algum número constante igual a $\sigma^2$.Ou seja, este postulado é a da homoscedasticidade, ou igual (homo) dispersão (scedasticidade), isto é, igual variância.


```{r, echo = TRUE, fig.height=4.5, fig.width=4.5}
x <- runif(100, 0, 6); 
y <- x + rnorm(100,  mean = 0, sd = .001 * x); 
plot(x, y); 
abline(lm(y ~ x))
```

---

##Livrar-se do espaço em branco pode ser útil

```{r, echo = TRUE, fig.height=4.5, fig.width=4.5}
plot(x, resid(lm(y ~ x))); 
abline(h = 0)
```

---

## Estimativa da variação residual

- Modelo $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ onde $\epsilon_i \sim N (0, \sigma ^ 2)$.
- A estimativa de ML de $\sigma ^ 2$ é $\frac {1} {n} \sum_ {i = 1} ^ n e_i ^ 2$, o quadrado médio residual.
- A maioria das pessoas usa

  $$
  \hat \sigma^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2.
  $$
  
- O $n-2$ em vez de $n$ é tal que $E [\hat \sigma ^ 2] = \sigma ^ 2$

---

## Exemplo Diamond 

```{r, echo = TRUE}
y <- diamond$price; x <- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
summary(fit)$sigma
sqrt(sum(resid(fit)^2) / (n - 2))
```

---

## Summarizando a variação

$$
\begin{align}
\sum_{i=1}^n (Y_i - \bar Y)^2 
& = \sum_{i=1}^n (Y_i - \hat Y_i + \hat Y_i - \bar Y)^2 \\
& = \sum_{i=1}^n (Y_i - \hat Y_i)^2 + 
2 \sum_{i=1}^n  (Y_i - \hat Y_i)(\hat Y_i - \bar Y) + 
\sum_{i=1}^n  (\hat Y_i - \bar Y)^2 \\
\end{align}
$$

****

### Rascunho

$(Y_i - \hat Y_i) = \{Y_i - (\bar Y - \hat \beta_1 \bar X) - \hat \beta_1 X_i\} = (Y_i - \bar Y) - \hat \beta_1 (X_i - \bar X)$

$(\hat Y_i - \bar Y) = (\bar Y - \hat \beta_1 \bar X - \hat \beta_1 X_i - \bar Y )
= \hat \beta_1  (X_i - \bar X)$

$\sum_{i=1}^n  (Y_i - \hat Y_i)(\hat Y_i - \bar Y) 
= \sum_{i=1}^n  \{(Y_i - \bar Y) - \hat \beta_1 (X_i - \bar X))\}\{\hat \beta_1  (X_i - \bar X)\}$

$=\hat \beta_1 \sum_{i=1}^n (Y_i - \bar Y)(X_i - \bar X) -\hat\beta_1^2\sum_{i=1}^n (X_i - \bar X)^2$

$= \hat \beta_1^2 \sum_{i=1}^n (X_i - \bar X)^2-\hat\beta_1^2\sum_{i=1}^n (X_i - \bar X)^2 = 0$

---


$$
\sum_{i=1}^n (Y_i - \bar Y)^2 
= \sum_{i=1}^n (Y_i - \hat Y_i)^2 + \sum_{i=1}^n  (\hat Y_i - \bar Y)^2 
$$

ou

Variação Total = Variação Residual + Variação de Regressão

Definir a porcentagem de variação total descrita pelo modelo como

$$
R^2 = \frac{\sum_{i=1}^n  (\hat Y_i - \bar Y)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}
= 1 - \frac{\sum_{i=1}^n  (Y_i - \hat Y_i)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}
$$

---

## Relação entre $R2$ e $r$ (a correlação)

Lembre-se que $(\hat Y_i - \bar Y) = \hat \beta_1  (X_i - \bar X)$
de modo a

$$
R^2 = \frac{\sum_{i=1}^n  (\hat Y_i - \bar Y)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}
= \hat \beta_1^2  \frac{\sum_{i=1}^n(X_i - \bar X)}{\sum_{i=1}^n (Y_i - \bar Y)^2}
= Cor(Y, X)^2
$$
Desde, recordar,

$$
\hat \beta_1 = Cor(Y, X)\frac{Sd(Y)}{Sd(X)}
$$
Então, $R^2$ é literalmente $r$ ao quadrado.

---

## Alguns fatos sobre $R^2$

- $ R^2$ é a porcentagem de variação explicada pelo modelo de regressão.
- $0 \leq R^2 \leq 1$
- $R^2$ é a correlação da amostra ao quadrado.
- $R^2$ pode ser um resumo enganoso do ajuste do modelo.
- A exclusão de dados pode aumentar $R^2$.
- (Para mais tarde.) Adicionar termos a um modelo de regressão sempre aumenta $R^2$.
- Do `exemplo (anscombe)` para ver os seguintes dados.
- Basicamente a mesma média e variância de X e Y.
- Correlações idênticas (daqui mesmo $R^2$).
- Mesma relação de regressão linear.

---

## `data(anscombe);example(anscombe)`

```{r, echo = FALSE, fig.height=5, fig.width=5, results='hide'}
require(stats); require(graphics); data(anscombe)
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  #print(anova(lmi))
}


## Agora, faça o que deveria ter feito em primeiro lugar: PLOTS
op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13))
  abline(mods[[i]], col = "blue")
}
mtext("Dados de regressão - Anscombe", outer = TRUE, cex = 1.5)
par(op)
```

